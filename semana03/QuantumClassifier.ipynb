{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Classifier\n",
    "\n",
    "Está dividido em 4 grandes blocos:\n",
    "\n",
    "## 1-Função de perda\n",
    "## 2-Codificação dos dados clássicos\n",
    "## 3-Tipo da rede neural\n",
    "## 4-Modelo de otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World!\n",
    "\n",
    "Template que atende 90% dos casos de uso. Está divido em 4 passos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-Carregamento dos dados para treinamento e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from QuantumClassifier import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris([0,1])\n",
    "\n",
    "seed = 1977\n",
    "training_inputs, test_inputs, \\\n",
    "training_labels, test_labels =  train_test_split(X, y, test_size = 0.66, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Hiperparâmetros do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lss = SquaredLoss()            # loss function type.                         [1-AbsoluteLoss, 2-CrossEntropyLoss, 3-SquaredLoss]\n",
    "enc = IBMQubitEncoding()       # encode classical data vector.               [1-IBMQubitEncoding, 2-IBMAmplitudeEncoding]\n",
    "qnn = IBM_TTN_Statevector(enc) # type / implementation of the neural network.[1-IBM_TTN_Statevector, 2-IBM_MERA_Statevector, 3-IBM_TTN_Simulator, 4-IBM_MERA_Simulator, 5-IBM_Bell_Statevector, 6-IBM_Bell_Simulator]\n",
    "mdl = Adam(qnn, lss, a=0.25, h=0.001, ct=0.025, it=25) # optimization model. [1-GradientDescent, 2-Adam]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Ajuste do modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametros iniciais:\n",
      " 2.166072250493227 2.2348796733443583 -1.5295659618414725 -0.04560961768358718 0.6629808141937388 0.02760938730712592 0.51365041415082\n",
      "Custo inicial: 0.392568\n",
      "Custo depois da iteração 5: 0.032532\n",
      "Custo final depois da iteração 7: 0.022134\n"
     ]
    }
   ],
   "source": [
    "params, t, c = mdl.fit(training_inputs, training_labels) # return model parameters for a given training dataset.\n",
    "print(\"Custo final depois da iteração %i: %f\" %(t, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-Predição:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia     : 98.4848%\n"
     ]
    }
   ],
   "source": [
    "prediction = qnn.predict(test_inputs, params) # return predictions for a given model and test dataset.\n",
    "print('Acurácia     : %.4f'%(100 - np.mean(np.abs(prediction - test_labels)) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstração de como usar o classificador para otimizar um circuito quântico.\n",
    "\n",
    "Para fazer a otimização precisamos escrever o circuito utilzando parâmetros ajustáveis, como o mostrado abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/Bell_State_circuit.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um circuito que gera os estados de Bell. Os dois primeiros $R_y$ codificam a entrada, que pode ser 0 ou 1 para cada qubit. Neste exemplo específico, $q_0=|0>$ e $q_1=|1>$. Os dois últimos $R_y$ são equivalentes ao Hadamard, que pode ser decomposto como $H=XY^{1/2}$. Essa escolha não é única, existem outras maneiras de codificar os dados e, também, construir o Hadamard (comumente com uma fase global envolvida).\n",
    "\n",
    "No exemplo usamos o computador \"ibmq_16_melbourne\", escolhido por ser o mais ruidoso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = load_bell_states()\n",
    "\n",
    "seed = 1977\n",
    "training_inputs2, test_inputs2, \\\n",
    "training_labels2, test_labels2 =  train_test_split(X2, y2, test_size = 0.66, random_state = seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lss = SquaredLoss()            # loss function type.\n",
    "enc = IBMQubitEncoding()       # encode classical data vector.\n",
    "qnn = IBM_Bell_Simulator(enc,'ibmq_16_melbourne')  # type / implementation of the neural network.\n",
    "mdl = Adam(qnn, lss, a=0.1, h=0.01, ct=0.0, it=100, adnit=10) # optimization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = qnn.predict(test_inputs2, [0]*qnn.params_vector_size(2)) # Here the circuit parameters are not changed.\n",
    "print('Custo antes da otimização    : %f' %mdl.cost(training_inputs2, training_labels2, [0]*qnn.params_vector_size(2)))\n",
    "print('Acurácia antes da otimização : %.4f'%(100 - np.mean(np.abs(prediction - test_labels2)) * 100) + '%')\n",
    "\n",
    "params, t, c = mdl.fit(training_inputs2, training_labels2) # return model parameters for a given training dataset.\n",
    "prediction = qnn.predict(test_inputs2, params) # return predictions for a given model and test dataset.\n",
    "print (\"Custo final depois da iteração %i: %f\" %(t, c))\n",
    "print('Acurácia após a otimização   : %.4f'%(100 - np.mean(np.abs(prediction - test_labels2)) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custo antes da otimização    : 0.029210\n",
    "\n",
    "Acurácia antes da otimização : 100.0000%\n",
    "\n",
    "Parametros iniciais:\n",
    "\n",
    " 2.9052288337392485 -1.325529482232365 2.069230892508073 -3.013069495602396\n",
    " \n",
    "Custo inicial: 0.176921\n",
    "\n",
    "Custo depois da iteração 5: 0.078836\n",
    "\n",
    "Custo depois da iteração 10: 0.039402\n",
    "\n",
    "Custo depois da iteração 15: 0.029964\n",
    "\n",
    "Custo depois da iteração 20: 0.031750\n",
    "\n",
    "Custo depois da iteração 25: 0.034099\n",
    "\n",
    "Custo final depois da iteração 17: 0.029781\n",
    "\n",
    "Acurácia após a otimização   : 100.0000%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/Simulations.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lss = SquaredLoss()            # loss function type.\n",
    "enc = IBMQubitEncoding()       # encode classical data vector.\n",
    "qnn = IBM_TTN_Simulator(enc, 'ibmq_london')           # type / implementation of the neural network.\n",
    "mdl = Adam(qnn, lss, a=0.25, h=0.01, ct=0.025, it=30) # optimization model.\n",
    "\n",
    "params, t, c = mdl.fit(training_inputs, training_labels) # return model parameters for a given training dataset.\n",
    "\n",
    "prediction = qnn.predict(test_inputs, params) # return predictions for a given model and test dataset.\n",
    "\n",
    "print('Custo final  :', c)\n",
    "print('Acurácia     : %.4f'%(100 - np.mean(np.abs(prediction - test_labels)) * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametros iniciais:\n",
    "\n",
    "-0.10480255  2.99346817 -1.94746787  1.74899496  1.14576659 -2.39546485  2.16811462\n",
    "\n",
    "Custo inicial: 0.138497\n",
    "\n",
    "Custo final  : 0.020219179882722742\n",
    "\n",
    "Acurácia     : 100.0000%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/ClassDiagram.png\" style=\"width:90%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hipótese\n",
    "## Codificação Automática de Dados Clássicos em Amplitudes de Estados de Emaranhados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hipótese é que os modelos de redes tensoriais, implementadas em circuitos quânticos, permitem que dados clássicos sejam compactados e codificados em amplitudes de estados emaranhados de maneira automática, bastando haver um prévio treinamento dos parâmetros da rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ver como isso pode ser feitos, observemos os seguintes circuitos TTN, usando respectivamente Qubit Encoding e Amplitude Encoding:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/TTN_Qubit.png\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/TTN_Amplitude.png\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sabemos, a partir dos testes previamente efetuados, que as redes TTN conseguem predizer corretamente quando os dados clássicos são codificados nas amplitudes de estados emaranhado. Esses testes utilizaram a função \"initialize\" do Qiskit.\n",
    "\n",
    "Observando os circuitos acima e interpretando a primeira parte do primeiro circuito e o initialize do segundo como caixas pretas, percebemos que o restante do circuito é igual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/TTN_Qubit_blackbox.png\" style=\"width:75%\">\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/israelferrazaraujo/comp-quantica/patch-3/projetos/2019-01/files/TTN_Amplitude_blackbox.png\" style=\"width:75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sendo os resultados finais iguais, podemos inferir que as caixas pretas são equivalentes. Ou seja, que a primeira parte do primeiro circuito está, na verdade, fazendo o mesmo que o \"initialize\", codificando os dados clássicos nas amplitiudes de um estado emaranhado!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
